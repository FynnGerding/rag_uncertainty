{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c339ecf",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2831b",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61f308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7422445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_project_root_cwd(root_dir_name: str, max_depth: int = 5) -> Optional[Path]:\n",
    "    try:\n",
    "        current_path = Path(os.getcwd())\n",
    "        original_cwd = current_path\n",
    "        \n",
    "        for _ in range(max_depth):\n",
    "            if current_path.name == root_dir_name:\n",
    "                os.chdir(current_path)\n",
    "                print(f\"CWD set to project root: {os.getcwd()}\")\n",
    "                return current_path\n",
    "            \n",
    "            if current_path == current_path.parent:\n",
    "                break\n",
    "            \n",
    "            current_path = current_path.parent\n",
    "            \n",
    "        raise FileNotFoundError(f\"Project root '{root_dir_name}' not found within {max_depth} levels above {original_cwd}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting CWD: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae13c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD set to project root: /vol/tensusers8/fgerding/rag_uncertainty\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/vol/tensusers8/fgerding/rag_uncertainty')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_project_root_cwd('rag_uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50cbadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "450 records loaded.\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"./results/results.json\")\n",
    "print(\"File exists:\", data_path.exists())\n",
    "\n",
    "# Load a small preview to confirm structure\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data), \"records loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc669fa4",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "UE_METRICS = [\n",
    "    \"semantic_entropy_global\",\n",
    "    \"sum_eigen\",\n",
    "]\n",
    "\n",
    "RAFE_METRICS = [\n",
    "    \"rafe_overall_score\",\n",
    "]\n",
    "\n",
    "ALL_METRICS = UE_METRICS + RAFE_METRICS\n",
    "OUTPUT_DIR = Path(\"data/analysis_outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463692e",
   "metadata": {},
   "source": [
    "#### Data Processing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframes(data):\n",
    "    \"\"\"Parses raw JSON-like data into structured DataFrames for analysis.\"\"\"\n",
    "    \n",
    "    # Example-level Data\n",
    "    example_rows = []\n",
    "    for i, rec in enumerate(data):\n",
    "        row = {\n",
    "            \"example_id\": i,\n",
    "            \"category\": rec.get(\"category\"),\n",
    "            \"question\": rec.get(\"question\"),\n",
    "            \"answer\": rec.get(\"answer\"),\n",
    "            \"answer_len\": len(rec.get(\"answer\", \"\")),\n",
    "            # Unpack metrics dynamically\n",
    "            **{k: rec.get(k) for k in ALL_METRICS},\n",
    "            # RAFE specific details\n",
    "            \"rafe_gen_supported\": rec.get(\"rafe_gen_supported\"),\n",
    "            \"rafe_gen_not_supported\": rec.get(\"rafe_gen_not_supported\"),\n",
    "            \"rafe_gen_irrelevant\": rec.get(\"rafe_gen_irrelevant\"),\n",
    "            \"rafe_gen_total_claims\": rec.get(\"rafe_gen_total_claims\"),\n",
    "        }\n",
    "        example_rows.append(row)\n",
    "    example_df = pd.DataFrame(example_rows)\n",
    "\n",
    "    # Claim-level Data\n",
    "    claim_rows = []\n",
    "    for i, rec in enumerate(data):\n",
    "        for j, c in enumerate(rec.get(\"rafe_details\") or []):\n",
    "            claim_rows.append({\n",
    "                \"example_id\": i,\n",
    "                \"claim_id\": j,\n",
    "                \"claim_text\": c.get(\"claim\"),\n",
    "                \"safe_label\": c.get(\"label\"),\n",
    "                \"n_evidence\": len(c.get(\"evidence\") or []),\n",
    "            })\n",
    "    claims_df = pd.DataFrame(claim_rows)\n",
    "\n",
    "    # Atomic Facts Data\n",
    "    atomic_rows = []\n",
    "    for i, rec in enumerate(data):\n",
    "        for j, txt in enumerate(rec.get(\"atomic_facts\") or []):\n",
    "            atomic_rows.append({\n",
    "                \"example_id\": i, \n",
    "                \"atomic_fact_id\": j, \n",
    "                \"atomic_fact_text\": txt\n",
    "            })\n",
    "    atomic_df = pd.DataFrame(atomic_rows)\n",
    "\n",
    "    return example_df, claims_df, atomic_df\n",
    "\n",
    "def check_consistency(data):\n",
    "    \"\"\"\n",
    "    Verifies data integrity by comparing reported summary scores \n",
    "    against recomputed counts from detailed logs.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for i, rec in enumerate(data):\n",
    "        labels = [c.get(\"label\") for c in (rec.get(\"safe_details\") or [])]\n",
    "        counts = {\n",
    "            \"supported\": labels.count(\"SUPPORTED\"),\n",
    "            \"not_supported\": labels.count(\"NOT_SUPPORTED\"),\n",
    "            \"irrelevant\": labels.count(\"IRRELEVANT\"),\n",
    "            \"total\": len(labels)\n",
    "        }\n",
    "        \n",
    "        row = {\"example_id\": i}\n",
    "        for key in counts:\n",
    "            # Handle edge case where total_claims is reported but others are None\n",
    "            reported = rec.get(f\"safe_gen_{key}\") \n",
    "            if key == \"total\" and reported is None:\n",
    "                reported = rec.get(\"safe_gen_total_claims\")\n",
    "            \n",
    "            # Default to 0 if None for calculation, strictly\n",
    "            reported_val = reported if reported is not None else 0\n",
    "            \n",
    "            row[f\"diff_{key}\"] = reported_val - counts[key]\n",
    "        records.append(row)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Check if all difference columns are 0\n",
    "    consistent_mask = (df[[\"diff_supported\", \"diff_not_supported\", \n",
    "                           \"diff_irrelevant\", \"diff_total\"]] == 0).all(axis=1)\n",
    "    \n",
    "    print(f\"Consistency Check: {consistent_mask.sum()}/{len(df)} examples fully consistent.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92eefd",
   "metadata": {},
   "source": [
    "#### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db5a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(df, metrics_x, metrics_y, label):\n",
    "    \"\"\"Computes Pearson and Spearman correlations between metric sets.\"\"\"\n",
    "    rows = []\n",
    "    seen_pairs = set()\n",
    "    \n",
    "    for x in metrics_x:\n",
    "        for y in metrics_y:\n",
    "            # Skip self-correlation and duplicate pairs (A-B vs B-A)\n",
    "            if x == y or (y, x) in seen_pairs: \n",
    "                continue\n",
    "            \n",
    "            sub = df[[x, y]].dropna()\n",
    "            if len(sub) < 3: # Insufficient data\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"type\": label,\n",
    "                \"metric_1\": x,\n",
    "                \"metric_2\": y,\n",
    "                \"pearson_r\": sub[x].corr(sub[y], method=\"pearson\"),\n",
    "                \"spearman_r\": sub[x].corr(sub[y], method=\"spearman\"),\n",
    "                \"n_samples\": len(sub)\n",
    "            })\n",
    "            \n",
    "            # If computing within the same list, mark pair as seen\n",
    "            if metrics_x is metrics_y:\n",
    "                seen_pairs.add((x, y))\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d7f3d",
   "metadata": {},
   "source": [
    "#### Visualization Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e39ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_scatter_plots(df, metrics, plot_dir):\n",
    "    \"\"\"\n",
    "    Generates scatter plots for all unique pairs of metrics.\n",
    "    Uses itertools.combinations to avoid duplicates and self-plots.\n",
    "    \"\"\"\n",
    "    plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for m1, m2 in itertools.combinations(metrics, 2):\n",
    "        sub = df[[m1, m2]].dropna()\n",
    "        if len(sub) < 3:\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(sub[m1], sub[m2], alpha=0.6, s=15, edgecolors='none')\n",
    "        plt.xlabel(m1)\n",
    "        plt.ylabel(m2)\n",
    "        plt.title(f\"{m1}\\nvs\\n{m2}\", fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_dir / f\"scatter_{m1}_vs_{m2}.pdf\")\n",
    "        plt.close()\n",
    "\n",
    "def plot_correlation_heatmap(df, metrics, save_path):\n",
    "    \"\"\"Generates a correlation matrix heatmap.\"\"\"\n",
    "    corr = df[metrics].corr(method='pearson')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, vmin=-1, vmax=1)\n",
    "    plt.title(\"Metric Correlation Matrix (Pearson)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_histograms(df, metrics, plot_dir):\n",
    "    \"\"\"Generates distribution histograms for all metrics.\"\"\"\n",
    "    for col in metrics:\n",
    "        sub = df[col].dropna()\n",
    "        if sub.empty: continue\n",
    "        \n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.hist(sub, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "        plt.title(f\"Distribution: {col}\", fontsize=10)\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_dir / f\"hist_{col}.pdf\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a70f0",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0458936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Analysis Pipeline...\n",
      "Consistency Check: 450/450 examples fully consistent.\n",
      "Data Loaded -> Examples: (450, 12), Claims: (2133, 5)\n",
      "Computing correlations...\n",
      "Generating plots...\n",
      "\n",
      "Summary of Correlations by Type\n",
      "            pearson_r  spearman_r\n",
      "type                             \n",
      "UE_vs_SAFE  -0.107894   -0.171139\n",
      "Within_UE    0.667426    0.810184\n",
      "\n",
      "Analysis complete. All outputs saved to: /vol/tensusers8/fgerding/rag_uncertainty/data/analysis_outputs\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Analysis Pipeline...\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_DIR = OUTPUT_DIR / \"plots\"\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Build Data\n",
    "ex_df, claim_df, atom_df = build_dataframes(data)\n",
    "cons_df = check_consistency(data)\n",
    "print(f\"Data Loaded -> Examples: {ex_df.shape}, Claims: {claim_df.shape}\")\n",
    "\n",
    "# 2. Compute Correlations (Between and Within groups)\n",
    "print(\"Computing correlations...\")\n",
    "corr_ue_safe = compute_correlations(ex_df, UE_METRICS, SAFE_METRICS, label=\"UE_vs_SAFE\")\n",
    "corr_within_ue = compute_correlations(ex_df, UE_METRICS, UE_METRICS, label=\"Within_UE\")\n",
    "corr_within_safe = compute_correlations(ex_df, SAFE_METRICS, SAFE_METRICS, label=\"Within_SAFE\")\n",
    "\n",
    "all_corrs = pd.concat([corr_ue_safe, corr_within_ue, corr_within_safe], ignore_index=True)\n",
    "\n",
    "# 3. Visualization\n",
    "print(\"Generating plots...\")\n",
    "generate_all_scatter_plots(ex_df, ALL_METRICS, PLOT_DIR)\n",
    "plot_histograms(ex_df, ALL_METRICS, PLOT_DIR)\n",
    "plot_correlation_heatmap(ex_df, ALL_METRICS, PLOT_DIR / \"correlation_heatmap.pdf\")\n",
    "\n",
    "# 4. Save Artifacts\n",
    "ex_df.to_csv(OUTPUT_DIR / \"examples.csv\", index=False)\n",
    "claim_df.to_csv(OUTPUT_DIR / \"claims.csv\", index=False)\n",
    "atom_df.to_csv(OUTPUT_DIR / \"atomic_facts.csv\", index=False)\n",
    "cons_df.to_csv(OUTPUT_DIR / \"consistency_report.csv\", index=False)\n",
    "all_corrs.to_csv(OUTPUT_DIR / \"correlations.csv\", index=False)\n",
    "\n",
    "print(\"\\nSummary of Correlations by Type\")\n",
    "print(all_corrs.groupby(\"type\")[[\"pearson_r\", \"spearman_r\"]].mean())\n",
    "print(f\"\\nAnalysis complete. All outputs saved to: {OUTPUT_DIR.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
